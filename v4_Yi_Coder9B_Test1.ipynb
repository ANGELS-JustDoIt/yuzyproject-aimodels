{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a5ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì‹¤í–‰ í™˜ê²½: CUDA\n"
     ]
    }
   ],
   "source": [
    "# [Cell 1] ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# ì„¤ì •ê°’\n",
    "INPUT_FILE = \"project_full_context.txt\"\n",
    "OUTPUT_JSON = \"project_flows.json\"\n",
    "\n",
    "# GPU í™•ì¸\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸš€ ì‹¤í–‰ í™˜ê²½: {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbf3691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì‹¤í–‰ í™˜ê²½: CUDA (GPU)\n",
      "ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (01-ai/Yi-Coder-9B-Chat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pak10\\.cache\\huggingface\\hub\\models--01-ai--Yi-Coder-9B-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [02:52<08:37, 172.55s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [05:53<05:54, 177.22s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [08:45<02:55, 175.11s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [10:22<00:00, 155.56s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:42<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì¤€ë¹„ ë)\n"
     ]
    }
   ],
   "source": [
    "# [Cell 2] Yi-Coder-9B-Chat ë¡œë”© (ì¸ì¦ ë¶ˆí•„ìš” / 4070 Ti 4ë¹„íŠ¸ ìµœì í™”)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# â˜… ëª¨ë¸ ë³€ê²½: ì¸ì¦ ì—†ê³  ì„±ëŠ¥ ì§±ì§±í•œ Yi-Coder\n",
    "MODEL_ID = \"01-ai/Yi-Coder-9B-Chat\"\n",
    "\n",
    "print(f\"ğŸš€ ì‹¤í–‰ í™˜ê²½: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# 1. 4ë¹„íŠ¸ ì–‘ìí™” (9B ëª¨ë¸ì„ 12GB VRAMì— ë„£ê¸° ìœ„í•´ í•„ìˆ˜)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_ID})\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\" # ì—ëŸ¬ ë°©ì§€\n",
    "    )\n",
    "    \n",
    "    # íŒ¨ë”© í† í° ì„¤ì •\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì¤€ë¹„ ë)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38adbcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê³„ì¸µí˜• ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# [Cell 3] í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € (ê³„ì¸µí˜• íŠ¸ë¦¬ êµ¬ì¡° ì „ìš©)\n",
    "\n",
    "# ìˆ˜ì •ëœ SYSTEM_PROMPT (DB ë° ë¯¸ë“¤ì›¨ì–´ ì¶”ì  ê°•í™”)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a 'Backend Code Analyst'.\n",
    "Your task is to generate a **Precise Call Graph** based ONLY on the provided code.\n",
    "\n",
    "### CRITICAL RULES (DO NOT HALLUCINATE)\n",
    "1. **Identify Database Driver**: Check `package.json` or imports. Is it `mongoose`, `mysql2`, or `pg`?\n",
    "   - If `mongoose` is used, DO NOT output SQL queries like `SELECT *`. Use Mongoose methods like `find()`, `save()`.\n",
    "2. **Trace Middleware**: If a router has `isAuth` or `validate`, you MUST include them in `children`.\n",
    "3. **Deep Trace**: Go from Router -> Middleware -> Controller -> Service/Data -> DB Library.\n",
    "\n",
    "### JSON OUTPUT FORMAT\n",
    "{\n",
    "  \"api\": [\n",
    "    {\n",
    "      \"category\": \"auth\",\n",
    "      \"categoryName\": \"Auth Feature\",\n",
    "      \"endpoints\": [\n",
    "        {\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"/auth/signup\",\n",
    "          \"function\": \"signup\",\n",
    "          \"file\": \"controller/auth.mjs\",\n",
    "          \"description\": \"User Signup\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"function\": \"validateSignup\",\n",
    "              \"file\": \"router/auth.mjs\",\n",
    "              \"description\": \"Validation Chain\",\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"function\": \"bcrypt.hashSync\",\n",
    "              \"file\": \"controller/auth.mjs\",\n",
    "              \"description\": \"Password Hashing\",\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"function\": \"createUser\",\n",
    "              \"file\": \"data/auth.mjs\",\n",
    "              \"description\": \"Save to DB\",\n",
    "              \"children\": [\n",
    "                {\n",
    "                   \"function\": \"User.save()\",\n",
    "                   \"file\": \"mongoose\",\n",
    "                   \"description\": \"MongoDB Insert\",\n",
    "                   \"children\": []\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… ê³„ì¸µí˜• ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0acc373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ 'project_full_context.txt' ì½ëŠ” ì¤‘...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m json_data = \u001b[43mrun_full_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mrun_full_scan\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     37\u001b[39m model_inputs = tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# ì¶”ë¡  ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ê²°ê³¼ ë””ì½”ë”©\u001b[39;00m\n\u001b[32m     48\u001b[39m generated_ids = [\n\u001b[32m     49\u001b[39m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     50\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2016\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2017\u001b[39m         input_ids=input_ids,\n\u001b[32m   2018\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2019\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2020\u001b[39m         **model_kwargs,\n\u001b[32m   2021\u001b[39m     )\n\u001b[32m   2023\u001b[39m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2024\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2036\u001b[39m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[32m   2037\u001b[39m     prepared_logits_warper = (\n\u001b[32m   2038\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_logits_warper(generation_config, device=input_ids.device)\n\u001b[32m   2039\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m generation_config.do_sample\n\u001b[32m   2040\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2041\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2971\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[39m\n\u001b[32m   2968\u001b[39m unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n\u001b[32m   2969\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._get_initial_cache_position(input_ids, model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(\n\u001b[32m   2972\u001b[39m     this_peer_finished, synced_gpus, device=input_ids.device, cur_len=cur_len, max_length=max_length\n\u001b[32m   2973\u001b[39m ):\n\u001b[32m   2974\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2975\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# [Cell 4] ì „ì²´ ì½”ë“œ ë¶„ì„ ë° JSON ìƒì„± ì—”ì§„ ëª¨ë¸ìš©\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"AI ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ\"\"\"\n",
    "    text = re.sub(r\"^```(json)?\", \"\", text.strip(), flags=re.MULTILINE)\n",
    "    text = re.sub(r\"```$\", \"\", text.strip(), flags=re.MULTILINE)\n",
    "    \n",
    "    # ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start == -1 or end == -1: return \"{}\"\n",
    "    return text[start:end+1]\n",
    "\n",
    "def run_full_scan():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"âŒ '{INPUT_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“‚ '{INPUT_FILE}' ì½ëŠ” ì¤‘...\")\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        full_code = f.read()\n",
    "        \n",
    "    code_context = full_code[:60000] \n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this code and generate Nested JSON:\\n\\n{code_context}\"}\n",
    "    ]\n",
    "\n",
    "    # Qwenìš© í…œí”Œë¦¿ ì ìš© ë° GPUë¡œ ì´ë™\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ì¶”ë¡  ì‹¤í–‰\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=4000,\n",
    "        temperature=0.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # ê²°ê³¼ ë””ì½”ë”©\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "# JSON íŒŒì‹± ë° ì €ì¥\n",
    "    json_str = extract_json(response)\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… ì™„ë²½í•œ JSON íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! -> {OUTPUT_JSON}\")\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âŒ JSON íŒŒì‹± ì‹¤íŒ¨. AI ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        print(response[:500])\n",
    "        return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "json_data = run_full_scan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c43c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ³ API Call Graph Analysis Result\n",
      "========================================\n",
      "â””â”€â”€ ğŸ“‚ Category: Auth Feature\n",
      "    â”œâ”€â”€ [POST] /auth/signup (User Signup)\n",
      "    â”‚   â”œâ”€â”€ Æ’ validateSignup - Validation Chain\n",
      "    â”‚   â”œâ”€â”€ Æ’ bcrypt.hashSync - Password Hashing\n",
      "    â”‚   â””â”€â”€ Æ’ createUser - Save to DB\n",
      "    â”‚       â””â”€â”€ Æ’ User.save() - MongoDB Insert\n",
      "    â”œâ”€â”€ [POST] /auth/login (User Login)\n",
      "    â”‚   â”œâ”€â”€ Æ’ validateLogin - Validation Chain\n",
      "    â”‚   â”œâ”€â”€ Æ’ bcrypt.compare - Password Verification\n",
      "    â”‚   â”œâ”€â”€ Æ’ createJwtToken - Create JWT Token\n",
      "    â”‚   â””â”€â”€ Æ’ res.status(200).json({ token, userid }) - Send JWT Token and User ID\n",
      "    â””â”€â”€ [GET] /auth/me (Get Current User Info)\n",
      "        â”œâ”€â”€ Æ’ authRepository.findById(req.id) - Find User by ID\n",
      "        â””â”€â”€ Æ’ res.status(200).json({ token, userid }) - Send JWT Token and User ID\n"
     ]
    }
   ],
   "source": [
    "# [Cell 5] í…ìŠ¤íŠ¸ íŠ¸ë¦¬ ë·°ì–´ (JSON êµ¬ì¡° í™•ì¸ìš©)\n",
    "\n",
    "def print_tree(node, prefix=\"\", is_last=True):\n",
    "    \"\"\"ì¬ê·€ì ìœ¼ë¡œ JSON íŠ¸ë¦¬ë¥¼ ì¶œë ¥\"\"\"\n",
    "    connector = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "    \n",
    "    # ë…¸ë“œ ì´ë¦„ ê²°ì • (function ë˜ëŠ” method+url)\n",
    "    if \"method\" in node:\n",
    "        name = f\"[{node['method']}] {node['url']} ({node.get('description', '')})\"\n",
    "    elif \"function\" in node:\n",
    "        name = f\"Æ’ {node['function']} - {node.get('description', '')}\"\n",
    "    elif \"category\" in node:\n",
    "        name = f\"ğŸ“‚ Category: {node.get('categoryName', node['category'])}\"\n",
    "    else:\n",
    "        name = \"Unknown Node\"\n",
    "\n",
    "    print(prefix + connector + name)\n",
    "    \n",
    "    # ìì‹ ë…¸ë“œ ìˆœíšŒ\n",
    "    children = node.get(\"children\", [])\n",
    "    if \"endpoints\" in node:\n",
    "        children = node[\"endpoints\"]\n",
    "        \n",
    "    count = len(children)\n",
    "    for i, child in enumerate(children):\n",
    "        new_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "        print_tree(child, new_prefix, i == count - 1)\n",
    "\n",
    "def visualize_json_structure():\n",
    "    if not os.path.exists(OUTPUT_JSON):\n",
    "        print(\"âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\"\\nğŸŒ³ API Call Graph Analysis Result\\n\" + \"=\"*40)\n",
    "    if \"api\" in data:\n",
    "        for cat in data[\"api\"]:\n",
    "            print_tree(cat)\n",
    "    else:\n",
    "        print(\"âš ï¸ 'api' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. JSON êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "visualize_json_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c60f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672ca19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e2801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86bfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea9ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734fd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe0718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
