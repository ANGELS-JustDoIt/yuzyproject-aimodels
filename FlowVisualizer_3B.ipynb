{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8a5ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì‹¤í–‰ í™˜ê²½: CUDA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# âš™ï¸ ì„¤ì •ê°’\n",
    "INPUT_FILE = \"project_full_context.txt\"   # ë¶„ì„í•  ì›ë³¸ ì½”ë“œ\n",
    "OUTPUT_JSON = \"project_flows.json\"        # LLMì´ ìƒì„±í•  íë¦„ ë°ì´í„°\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "# GPU ì„¤ì • í™•ì¸\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸš€ ì‹¤í–‰ í™˜ê²½: {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a50f355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘... (Qwen/Qwen2.5-Coder-1.5B-Instruct)\n",
      "âœ… ëª¨ë¸ ì¥ì°© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘... ({MODEL_ID})\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ì¥ì°© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38adbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "You are a 'Code Flow Visualization Expert' and Senior Developer.\n",
    "Analyze the provided code and extract **execution flows** (Call Graph).\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1. Identify which function calls which function.\n",
    "2. Extract the **exact code snippet** where each call happens.\n",
    "3. Keep descriptions simple for beginners.\n",
    "\n",
    "### OUTPUT FORMAT (JSON ONLY)\n",
    "{\n",
    "  \"flows\": \"signup\",\n",
    "  \"bundle\": \"\"\"\n",
    "app.use(\"/auth\", authRouter);\n",
    "\n",
    "router.post(\"/signup\", validateSignup, authController.signup);\n",
    "\n",
    "export async function signup(req, res, next) {\n",
    "  const { userid, password, name, email, url } = req.body;\n",
    "\n",
    "  //íšŒì› ì¤‘ë³µ ì²´í¬í•˜ê¸°\n",
    "  const found = await authRepository.findByUserid(userid);\n",
    "  if (found) {\n",
    "    return res.status(409).json({ message: `${userid}ì´ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤.` });\n",
    "  }\n",
    "  const hashed = bcrypt.hashSync(password, config.bcrypt.saltRounds);\n",
    "  const user = await authRepository.createUser({\n",
    "    userid,\n",
    "    password: hashed,\n",
    "    name,\n",
    "    email,\n",
    "    url,\n",
    "  });\n",
    "\n",
    "  //   const user = await authRepository.createUser(userid, password, name, email);\n",
    "  const token = await createJwtToken(user);\n",
    "  console.log(token);\n",
    "  res.status(201).json({ token, userid });\n",
    "}\n",
    "\n",
    "export async function createUser(user) {\n",
    "  return new User(user).save().then((data) => data.id);\n",
    "}\"\"\"\n",
    "}\n",
    "\n",
    "### RULES\n",
    "- Output **JSON ONLY**. No markdown.\n",
    "- The `code_snippet` MUST be exact and exist in the code.\n",
    "'''\n",
    "\n",
    "# ì²­í¬ ë‹¨ìœ„ í”„ë¡¬í”„íŠ¸ (íŒŒì¼ë³„ ë¶„ì„)\n",
    "CHUNK_PROMPT = \"\"\"\n",
    "Analyze this code file and extract function calls.\n",
    "Focus on: {user_query}\n",
    "\n",
    "File: {filename}\n",
    "Code:\n",
    "{code_snippet}\n",
    "\n",
    "Return JSON with flows found in this file only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d313b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files_from_context(code_context):\n",
    "    \"\"\"\n",
    "    project_full_context.txtì—ì„œ íŒŒì¼ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "    í˜•ì‹: === FILE_PATH: filename ===\n",
    "    \"\"\"\n",
    "    files = {}\n",
    "    pattern = r\"=== FILE_PATH:\\s*(.+?)\\s*===\\n(.*?)(?===\\s*FILE_PATH:|$)\"\n",
    "    matches = re.finditer(pattern, code_context, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        filename = match.group(1).strip()\n",
    "        content = match.group(2).strip()\n",
    "        files[filename] = content\n",
    "    \n",
    "    if not files:\n",
    "        # íŒŒì¼ êµ¬ë¶„ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ ê°„ì£¼\n",
    "        files[\"<full_context>\"] = code_context\n",
    "    \n",
    "    return files\n",
    "\n",
    "def analyze_chunk(filename, code_snippet, user_query):\n",
    "    \"\"\"\n",
    "    ê°œë³„ íŒŒì¼/ì²­í¬ë¥¼ LLMìœ¼ë¡œ ë¶„ì„\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": CHUNK_PROMPT.format(\n",
    "                filename=filename,\n",
    "                code_snippet=code_snippet,\n",
    "                user_query=user_query\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(f\"  ğŸ” ë¶„ì„ ì¤‘: {filename} ({len(code_snippet)} chars)\")\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1028,  # ì²­í¬ì´ë¯€ë¡œ ë” ì§§ê²Œ\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # ì •ì œ\n",
    "    clean_text = response_text.strip()\n",
    "    clean_text = re.sub(r\"^```(json)?\", \"\", clean_text, flags=re.MULTILINE).strip()\n",
    "    clean_text = re.sub(r\"```$\", \"\", clean_text, flags=re.MULTILINE).strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(clean_text)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"    âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨\")\n",
    "        return {\"flows\": []}\n",
    "\n",
    "def analyze_code_flow_chunked(user_query):\n",
    "    \"\"\"\n",
    "    íŒŒì¼ ë‹¨ìœ„ ì²­í¬ ë¶„ì„ + í†µí•©\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"âŒ '{INPUT_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return False\n",
    "\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        code_context = f.read()\n",
    "\n",
    "    # 1. íŒŒì¼ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "    files = parse_files_from_context(code_context)\n",
    "    print(f\"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(files)}ê°œ\")\n",
    "\n",
    "    # 2. ê° íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„\n",
    "    all_flows = []\n",
    "    for filename, content in files.items():\n",
    "        if len(content.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ íŒŒì¼ ìŠ¤í‚µ\n",
    "            print(f\"  â­ï¸  ìŠ¤í‚µ: {filename} (ë„ˆë¬´ ì§§ìŒ)\")\n",
    "            continue\n",
    "\n",
    "        result = analyze_chunk(filename, content, user_query)\n",
    "        flows = result.get(\"flows\", [])\n",
    "        \n",
    "        # íŒŒì¼ëª… ì¶”ê°€\n",
    "        for flow in flows:\n",
    "            flow[\"source_file\"] = filename\n",
    "            all_flows.append(flow)\n",
    "        \n",
    "        print(f\"    âœ… {len(flows)}ê°œ flow ë°œê²¬\")\n",
    "\n",
    "    # 3. í†µí•© JSON ì €ì¥\n",
    "    integrated_data = {\"flows\": all_flows, \"total_flows\": len(all_flows)}\n",
    "    \n",
    "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(integrated_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… í†µí•© JSON ì €ì¥: {OUTPUT_JSON}\")\n",
    "    print(f\"ğŸ“Š ì´ {len(all_flows)}ê°œ flow ì¶”ì¶œë¨\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ë³„ë„ ì…€: ì‹œê°í™” ìƒì„±\n",
    "def visualize_flow_arrows_enhanced():\n",
    "    \"\"\"\n",
    "    JSON ë°ì´í„°ë¥¼ í–¥ìƒëœ ì´ë¯¸ì§€ë¡œ ì‹œê°í™”\n",
    "    \"\"\"\n",
    "    if not os.path.exists(OUTPUT_JSON) or not os.path.exists(INPUT_FILE):\n",
    "        print(\"âŒ ì‹œê°í™”í•  ë°ì´í„° íŒŒì¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            flows = data.get(\"flows\", [])\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨\")\n",
    "        return None\n",
    "\n",
    "    if not flows:\n",
    "        print(\"âš ï¸ ì‹œê°í™”í•  flowê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    # ì´ë¯¸ì§€ í¬ê¸° ê³„ì‚°\n",
    "    img_width = 1400\n",
    "    flow_height = 150  # ê° flowë‹¹ ë†’ì´\n",
    "    img_height = max(800, len(flows) * flow_height + 200)\n",
    "    \n",
    "    img = Image.new('RGB', (img_width, img_height), color='#1e1e1e')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        font_small = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "        font_medium = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "        font_title = ImageFont.truetype(\"arial.ttf\", 18)\n",
    "    except:\n",
    "        font_small = ImageFont.load_default()\n",
    "        font_medium = font_small\n",
    "        font_title = font_small\n",
    "\n",
    "    # ì œëª©\n",
    "    draw.text((50, 30), \"ğŸ“Š Code Flow Visualization\", fill=\"#FFFFFF\", font=font_title)\n",
    "    draw.text((50, 60), f\"Total Flows: {len(flows)}\", fill=\"#ABB2BF\", font=font_medium)\n",
    "    \n",
    "    colors = [\"#FF5555\", \"#50FA7B\", \"#8BE9FD\", \"#FFB86C\", \"#BD93F9\", \"#FF79C6\"]\n",
    "    y_offset = 120\n",
    "\n",
    "    print(f\"ğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘... ({len(flows)} flows)\")\n",
    "\n",
    "    for flow_idx, flow in enumerate(flows):\n",
    "        flow_color = colors[flow_idx % len(colors)]\n",
    "        \n",
    "        # Flow ë°°ê²½\n",
    "        draw.rectangle([40, y_offset-10, img_width-40, y_offset+flow_height-20], \n",
    "                       outline=flow_color, width=2)\n",
    "        \n",
    "        # Flow ì œëª©\n",
    "        flow_name = flow.get('flow_name', 'Unknown')\n",
    "        source_file = flow.get('source_file', 'N/A')\n",
    "        draw.text((60, y_offset+5), f\"Flow {flow_idx+1}: {flow_name}\", \n",
    "                  fill=flow_color, font=font_medium)\n",
    "        draw.text((60, y_offset+30), f\"ğŸ“ {source_file}\", \n",
    "                  fill=\"#ABB2BF\", font=font_small)\n",
    "        \n",
    "        sequence = flow.get('sequence', [])\n",
    "        \n",
    "        # Sequence ë‹¨ê³„ë³„ í‘œì‹œ\n",
    "        step_y = y_offset + 60\n",
    "        for i, step in enumerate(sequence):\n",
    "            step_color = colors[(flow_idx + i) % len(colors)]\n",
    "            \n",
    "            f_word = step['from'].get('focus_word', '?')\n",
    "            t_word = step['to'].get('focus_word', '?')\n",
    "            desc = step.get('description', '')[:60]\n",
    "            \n",
    "            # ë‹¨ê³„ ë²ˆí˜¸\n",
    "            draw.text((70, step_y), f\"{i+1}.\", fill=step_color, font=font_medium)\n",
    "            \n",
    "            # í˜¸ì¶œ ê´€ê³„\n",
    "            draw.text((100, step_y), f\"{f_word}\", fill=\"#50FA7B\", font=font_small)\n",
    "            draw.text((220, step_y), \"â†’\", fill=step_color, font=font_medium)\n",
    "            draw.text((245, step_y), f\"{t_word}\", fill=\"#8BE9FD\", font=font_small)\n",
    "            \n",
    "            # ì„¤ëª…\n",
    "            draw.text((400, step_y), desc, fill=\"#ABB2BF\", font=font_small)\n",
    "            \n",
    "            step_y += 25\n",
    "        \n",
    "        y_offset += flow_height\n",
    "\n",
    "    return img\n",
    "\n",
    "# ì‹œê°í™” ì‹¤í–‰\n",
    "print(\"ğŸ¨ ì‹œê°í™” ì‹œì‘...\")\n",
    "result_img = visualize_flow_arrows_enhanced()\n",
    "\n",
    "if result_img:\n",
    "    display(result_img)\n",
    "    result_img.save(\"final_flow_chart_chunked.png\")\n",
    "    print(\"âœ¨ ì‹œê°í™” ì™„ë£Œ! ì €ì¥: final_flow_chart_chunked.png\")\n",
    "else:\n",
    "    print(\"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa8ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹œì‘...\n",
      "ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: 1ê°œ\n",
      "  ğŸ” ë¶„ì„ ì¤‘: <full_context> (12721 chars)\n",
      "    âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨\n",
      "    âœ… 0ê°œ flow ë°œê²¬\n",
      "âœ… í†µí•© JSON ì €ì¥: project_flows.json\n",
      "ğŸ“Š ì´ 0ê°œ flow ì¶”ì¶œë¨\n",
      "\n",
      "ğŸ“‹ === ì¶”ì¶œëœ íë¦„ (JSON) ===\n",
      "\n",
      "{\n",
      "  \"flows\": [],\n",
      "  \"total_flows\": 0\n",
      "}\n",
      "\n",
      "âœ… JSON ì €ì¥ ì™„ë£Œ: project_flows.json\n",
      "ğŸ“Š ì´ 0ê°œ flow ë°œê²¬\n"
     ]
    }
   ],
   "source": [
    "# 1. ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰\n",
    "print(\"ğŸš€ ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹œì‘...\")\n",
    "success = analyze_code_flow_chunked(user_query=\"íšŒì›ê°€ì…(Signup) ì‹œ ë°ì´í„° ì´ë™ íë¦„\")\n",
    "\n",
    "# 2. ì„±ê³µ ì‹œ JSON ê²°ê³¼ ì¶œë ¥\n",
    "if success:\n",
    "    with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ === ì¶”ì¶œëœ íë¦„ (JSON) ===\\n\")\n",
    "    print(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    print(f\"\\nâœ… JSON ì €ì¥ ì™„ë£Œ: {OUTPUT_JSON}\")\n",
    "    print(f\"ğŸ“Š ì´ {data.get('total_flows', 0)}ê°œ flow ë°œê²¬\")\n",
    "    \n",
    "    # ê° flowë³„ ìš”ì•½\n",
    "    for idx, flow in enumerate(data.get('flows', []), 1):\n",
    "        print(f\"\\n[Flow {idx}] {flow.get('flow_name', 'Unknown')}\")\n",
    "        print(f\"  íŒŒì¼: {flow.get('source_file', 'Unknown')}\")\n",
    "        sequence = flow.get('sequence', [])\n",
    "        print(f\"  ë‹¨ê³„: {len(sequence)}ê°œ\")\n",
    "        for step in sequence:\n",
    "            from_word = step['from'].get('focus_word', '?')\n",
    "            to_word = step['to'].get('focus_word', '?')\n",
    "            desc = step.get('description', '')\n",
    "            print(f\"    â†’ {from_word} â†’ {to_word}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc320dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040852ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  AI(3B) ë¶„ì„ ì‹œì‘... Query: 'íšŒì›ê°€ì…(Signup) ì‹œ ë°ì´í„° ì´ë™ íë¦„'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. ë¶„ì„ ì‹¤í–‰\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m success = \u001b[43manalyze_code_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43míšŒì›ê°€ì…(Signup) ì‹œ ë°ì´í„° ì´ë™ íë¦„\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. ì„±ê³µ ì‹œ ì‹œê°í™”\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36manalyze_code_flow\u001b[39m\u001b[34m(user_query)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ§  AI(3B) ë¶„ì„ ì‹œì‘... Query: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 4. ìƒì„± (Inference)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ì¶©ë¶„í•œ ê¸¸ì´ í™•ë³´\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# ì •í™•ë„ ì¤‘ì‹œ\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 5. ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ í† í° ì´í›„ë§Œ ì˜ë¼ë‚´ê¸°)\u001b[39;00m\n\u001b[32m     36\u001b[39m generated_ids = [\n\u001b[32m     37\u001b[39m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2779\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2777\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2779\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2780\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2781\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
