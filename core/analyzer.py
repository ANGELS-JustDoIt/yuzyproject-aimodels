# project_root/core/analyzer.py
import os
import json
import re
import time
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ëŒ€ìƒ í´ë” ê²½ë¡œ ì„¤ì •(ì‹¤ì œ ë°±ì—”ë“œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
TARGET_FOLDER_PATH = r"C:\Pyg\Projects\semi\yuzyproject-aimodels\server"

# ê²°ê³¼ë¬¼ì„ ì €ì¥í•  íŒŒì¼ëª…
OUTPUT_FILENAME = "project_full_context.txt"

# ìˆ˜ì§‘í•  íŒŒì¼ í™•ì¥ì
TARGET_EXTENSIONS = [
    '.mjs', '.js', '.ts', '.py', '.java', '.go', '.json', '.yaml', '.yml',
    '.sh', '.rb', '.php', '.html', '.css', '.scss', '.md', '.jsx', '.tsx'
]

IGNORE_DIRS = {
    'node_modules', 'venv', '.git', '__pycache__',
    'dist', 'build', '.idea', '.vscode', 'coverage',
    'frontend', 'front', 'client', 'web',
}


# ì œì™¸í•  íŒŒì¼ (íŒŒì¼ ì´ë¦„ë§Œ ì •í™•íˆ)
IGNORE_FILES = {
    'package-lock.json',
    'yarn.lock',
    '.DS_Store'
}

# ì„¤ì •ê°’ (ìœ ì§€)
INPUT_FILE = "project_full_context.txt"
OUTPUT_JSON = "project_flows.json"

# ëª¨ë¸ ID (ìœ ì§€)
MODEL_ID = "Qwen/Qwen2.5-Coder-1.5B-Instruct"


# =========================
# (ìœ ì§€) ë„¤ í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ
# =========================
SYSTEM_PROMPT = """
You are a 'Senior Backend Architect'.
Your task is to generate a **Deep & Precise Call Graph** JSON.

### ğŸš¨ CRITICAL RULES (DO NOT BE LAZY)
1. **NEVER Leave Children Empty**: You MUST trace down to the **Service/Repository** and **Database (Mongoose)** layers.
   - Bad: `"children": []`
   - Good: `"children": [{"function": "repository...", "children": [{"function": "User.find..."}]}]`
2. **Trace Middleware**: If a route has `isAuth`, add it as the FIRST child node.
3. **Analyze Logic**: 
   - `signup`: findByUserid -> bcrypt -> User.save.
   - `post`: isAuth -> controller -> repository -> Post.find/save.
4. **No Recursion**: A function (`login`) CANNOT call itself (`login`).

### ONE-SHOT EXAMPLE (Follow this depth strictly!)
Input Code: 
`router.post('/post', isAuth, createPost)`
`function createPost() { ... postRepository.create(...) }`
`function create() { ... new Post(...).save() }`

Output JSON:
{
  "category": "post",
  "endpoints": [
    {
      "method": "POST",
      "url": "/post",
      "function": "createPost",
      "children": [
        { "function": "isAuth", "file": "middleware/auth.mjs", "description": "Auth Check", "children": [] },
        { "function": "create", "file": "data/post.mjs", "description": "Repository Logic", "children": [
            { "function": "Post.save()", "file": "mongoose", "description": "DB Insert", "children": [] }
          ] 
        }
      ]
    }
  ]
}

### JSON OUTPUT FORMAT
Return ONLY valid JSON. Structure:
{
  "api": [
    { "category": "auth", "categoryName": "Auth Feature", "endpoints": [...] },
    { "category": "post", "categoryName": "Post Feature", "endpoints": [...] }
  ]
}
"""


# =========================
# ìµœì í™” ì„¤ì •
# =========================
@dataclass
class AnalyzerConfig:
    # â€œì›ë˜ 500~1700ì¤„ ë¶„ì„í–ˆë‹¤â€ ìš”êµ¬ ë°˜ì˜ (ê¸°ë³¸ 1700ì¤„)
    max_total_lines: int = 1700
    # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ë¬¸ì ìˆ˜ ì œí•œ(ë„ˆê°€ ì“°ë˜ 50,000ì ê°ê° ìœ ì§€)
    max_total_chars: int = 50_000

    # ìƒì„± ì œì–´: 1ë¶„ ë‚´ ëª©í‘œ (ê¸°ë³¸ 55ì´ˆ ì œí•œ)
    max_time_seconds: int = 200
    max_new_tokens: int = 4096  # 2048ë³´ë‹¤ ì •í™•ë„ ë” ë‚˜ì˜¤ëŠ”ë°, ì‹œê°„ ì œí•œì´ ìˆìœ¼ë‹ˆ ì•ˆì „

    # ë°˜ë³µ ë°©ì§€ (ìœ ì§€)
    repetition_penalty: float = 1.1


# =========================
# ëª¨ë¸ ìºì‹œ(ì„œë²„ì—ì„œ ì¬ì‚¬ìš©í•˜ë ¤ê³  ì „ì—­ ì‹±ê¸€í„´)
# =========================
_TOKENIZER = None
_MODEL = None


def _init_torch_perf():
    # GPUë©´ TF32 í—ˆìš©(ì†ë„â†‘, í’ˆì§ˆ í° ì°¨ì´ ì—†ìŒ)
    if torch.cuda.is_available():
        try:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
        except Exception:
            pass


def load_model_once():
    global _TOKENIZER, _MODEL

    if _TOKENIZER is not None and _MODEL is not None:
        return _TOKENIZER, _MODEL

    _init_torch_perf()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ ì‹¤í–‰ í™˜ê²½: {device.upper()}")
    print(f"ğŸš€ ì‹¤í–‰ í™˜ê²½: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}")
    print("âœ… Qwen 1.5B ìµœì¢… í”„ë¡¬í”„íŠ¸(Post ê¹Šì´ ê°•í™”) ì„¤ì • ì™„ë£Œ")

    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_ID})")

    # torch_dtype ê²½ê³  ì—†ì• ë ¤ê³  dtype ì‚¬ìš©
    _TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

    _MODEL = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True
    )

    if _TOKENIZER.pad_token is None:
        _TOKENIZER.pad_token = _TOKENIZER.eos_token

    _MODEL.eval()
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    return _TOKENIZER, _MODEL


# =========================
# íŒŒì¼ ìˆ˜ì§‘(ì†ë„/í† í° ìµœì í™” í•µì‹¬)
# =========================
def _should_ignore_dir(dirname: str) -> bool:
    return dirname in IGNORE_DIRS


def _should_collect_file(filename: str) -> bool:
    if filename in IGNORE_FILES:
        return False
    _, ext = os.path.splitext(filename)
    return ext.lower() in TARGET_EXTENSIONS


def _read_file_lines(path: str) -> List[str]:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.readlines()


def build_project_full_context(
    target_folder: str,
    output_file: str,
    cfg: AnalyzerConfig
) -> Dict[str, Any]:
    """
    - ë„¤ ìš”êµ¬: í´ë” -> í™•ì¥ì ê¸°ë°˜ ìˆ˜ì§‘ -> í•˜ë‚˜ì˜ í…ìŠ¤ì³ íŒŒì¼
    - ìµœì í™”: max_total_lines / max_total_chars ë‚´ì—ì„œë§Œ ëˆ„ì (ì˜ˆì „ 500~1700ì¤„ ê°ê° ì¬í˜„)
    """
    if not os.path.exists(target_folder):
        raise FileNotFoundError(f"âŒ ëŒ€ìƒ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {target_folder}")

    print("âœ… ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - ëŒ€ìƒ í´ë”: {target_folder}")
    print(f"   - ìˆ˜ì§‘ í™•ì¥ì: {TARGET_EXTENSIONS}")

    print(f"ğŸ“¦ í´ë” ìŠ¤ìº” ì‹œì‘: {target_folder}")

    chunks: List[str] = []
    included_files = 0
    skipped_files = 0
    total_lines = 0
    total_chars = 0

    for root, dirs, files in os.walk(target_folder):
        dirs[:] = [d for d in dirs if not _should_ignore_dir(d)]

        for file in files:
            if not _should_collect_file(file):
                skipped_files += 1
                continue

            full_path = os.path.join(root, file)
            rel_path = os.path.relpath(full_path, target_folder)

            try:
                lines = _read_file_lines(full_path)

                # ë‚¨ì€ ì˜ˆì‚° ê³„ì‚°
                if total_lines >= cfg.max_total_lines or total_chars >= cfg.max_total_chars:
                    break

                remain_lines = cfg.max_total_lines - total_lines
                # íŒŒì¼ ë¼ì¸ ì¼ë¶€ë§Œ ì·¨í•¨
                take_lines = lines[:max(0, remain_lines)]

                content = "".join(take_lines)

                # ë¬¸ì ì˜ˆì‚°ë„ ì ìš©
                remain_chars = cfg.max_total_chars - total_chars
                if len(content) > remain_chars:
                    content = content[:max(0, remain_chars)]

                block = "\n".join([
                    "===== FILE START =====",
                    f"PATH: {rel_path}",
                    "----- CODE -----",
                    content.rstrip("\n"),
                    "===== FILE END =====",
                    ""
                ])

                # ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                added_lines = content.count("\n") + 1 if content else 0
                added_chars = len(content)

                # í˜¹ì‹œë¼ë„ 0ì´ë©´ ìŠ¤í‚µ
                if added_lines == 0:
                    skipped_files += 1
                    continue

                chunks.append(block)
                included_files += 1
                total_lines += added_lines
                total_chars += added_chars

            except Exception:
                skipped_files += 1
                continue

        # ìƒìœ„ ë£¨í”„ë„ ì¤‘ë‹¨ ì¡°ê±´ ì²´í¬
        if total_lines >= cfg.max_total_lines or total_chars >= cfg.max_total_chars:
            break

    merged = "\n".join(chunks)
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(merged)

    print("âœ… í…ìŠ¤ì³ íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    print(f"   - ì €ì¥ ê²½ë¡œ: {output_file}")
    print(f"   - í¬í•¨ íŒŒì¼ ìˆ˜: {included_files}")
    print(f"   - ìŠ¤í‚µ íŒŒì¼ ìˆ˜: {skipped_files}")
    print(f"   - ëˆ„ì  ë¼ì¸ ìˆ˜(ëŒ€ëµ): {total_lines}")
    print(f"   - ëˆ„ì  ë¬¸ì ìˆ˜: {total_chars}")

    return {
        "output_file": output_file,
        "included_files": included_files,
        "skipped_files": skipped_files,
        "total_lines": total_lines,
        "total_chars": total_chars,
    }


# =========================
# LLM ë¶„ì„
# =========================
def _extract_json(text: str) -> str:
    text = re.sub(r"^```(json)?", "", text.strip(), flags=re.MULTILINE)
    text = re.sub(r"```$", "", text.strip(), flags=re.MULTILINE)
    start = text.find('{')
    end = text.rfind('}')
    if start == -1 or end == -1:
        return "{}"
    return text[start:end + 1]


def _generate_once(tokenizer, model, messages: List[Dict[str, str]], cfg: AnalyzerConfig) -> str:
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([prompt], return_tensors="pt").to(model.device)

    with torch.inference_mode():
        out = model.generate(
            **model_inputs,
            max_new_tokens=cfg.max_new_tokens,
            do_sample=False,
            num_beams=1,
            repetition_penalty=cfg.repetition_penalty,
            use_cache=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            max_time=cfg.max_time_seconds,  # 1ë¶„ ë‚´ ëª©í‘œ
        )

    gen_ids = out[0][model_inputs.input_ids.shape[1]:]
    return tokenizer.decode(gen_ids, skip_special_tokens=True)


def analyze_to_json(cfg: Optional[AnalyzerConfig] = None) -> Dict[str, Any]:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸:
    1) í´ë” -> project_full_context.txt ìƒì„±
    2) ëª¨ë¸ ë¡œë”©(1íšŒ ìºì‹œ)
    3) ë¶„ì„ -> project_flows.json ì €ì¥
    """
    if cfg is None:
        cfg = AnalyzerConfig()

    # 1) í…ìŠ¤ì³ ìƒì„±
    build_project_full_context(TARGET_FOLDER_PATH, OUTPUT_FILENAME, cfg)

    # INPUT_FILE ì´ë¦„ ìœ ì§€(ë„¤ ìš”êµ¬)
    if OUTPUT_FILENAME != INPUT_FILE:
        # ë„ˆê°€ â€œì ˆëŒ€ íŒŒì¼ëª… ë¬´ì‹œí•˜ì§€ ë§ë¼â€ê³  í•´ì„œ ì—¬ê¸°ì„œ ê°•ì œ ì¼ì¹˜ì‹œí‚¤ì§€ ì•Šê³  ê²½ê³ ë§Œ
        print("âš ï¸ ê²½ê³ : OUTPUT_FILENAMEê³¼ INPUT_FILEëª…ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
        print(f"   - OUTPUT_FILENAME: {OUTPUT_FILENAME}")
        print(f"   - INPUT_FILE: {INPUT_FILE}")

    if not os.path.exists(INPUT_FILE):
        # í˜¹ì‹œ OUTPUT_FILENAMEë§Œ ìƒì„±ëê³  INPUT_FILEì´ ë‹¤ë¥´ë©´ ì—¬ê¸°ì„œ ë§‰íˆë‹ˆê¹Œ ì•ˆë‚´
        raise FileNotFoundError(f"âŒ íŒŒì¼ ì—†ìŒ: {INPUT_FILE} (OUTPUT_FILENAME={OUTPUT_FILENAME} ìƒì„±ë¨)")

    # 2) ëª¨ë¸ ë¡œë”©(ìºì‹œ)
    tokenizer, model = load_model_once()

    # 3) ì½”ë“œ ë¡œë“œ (ì˜ˆì „ ë°©ì‹: í° íŒŒì¼ ì „ì²´ X, ì§€ê¸ˆì€ ì´ë¯¸ ë¼ì¸/ë¬¸ì ì œí•œëœ í…ìŠ¤ì³)
    print(f"ğŸ“‚ ì½”ë“œ ë¶„ì„ ì‹œì‘: '{INPUT_FILE}'")
    with open(INPUT_FILE, "r", encoding="utf-8", errors="ignore") as f:
        code_context = f.read()

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"Analyze the source code to visualize the logic flow. Generate the Nested JSON Structure:\n\n{code_context}"}
    ]

    print("ğŸ§  Qwen 1.5Bê°€ 'ê³ ì† ì •ë°€ ëª¨ë“œ(Greedy)'ë¡œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ë¹ ë¦„ & ê²°ê³¼ ê³ ì •)")
    t0 = time.time()
    response = _generate_once(tokenizer, model, messages, cfg)
    dt = time.time() - t0

    json_str = _extract_json(response)

    # 4) 1ì°¨ íŒŒì‹± ì‹œë„
    try:
        data = json.loads(json_str)
        with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ë¶„ì„ ì™„ë£Œ! -> {OUTPUT_JSON} ({dt:.1f}s)")
        return data
    except Exception:
        # 5) ì‹¤íŒ¨ ì‹œ: â€œJSONë§Œ ê³ ì³ë¼â€ ë³µêµ¬ íŒ¨ìŠ¤ (ì…ë ¥ì´ ì•„ì£¼ ì§§ì•„ì ¸ì„œ ë¹ ë¦„)
        print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ë³µêµ¬ íŒ¨ìŠ¤(ì§§ì€ ì…ë ¥)ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        repair_messages = [
            {"role": "system", "content": "Return ONLY valid JSON. Do not add any commentary."},
            {"role": "user", "content": f"Fix this into valid JSON only:\n\n{json_str[:8000]}"}
        ]

        repair_cfg = AnalyzerConfig(
            max_total_lines=cfg.max_total_lines,
            max_total_chars=cfg.max_total_chars,
            max_time_seconds=min(20, cfg.max_time_seconds),
            max_new_tokens=2048,
            repetition_penalty=cfg.repetition_penalty
        )

        repaired = _generate_once(tokenizer, model, repair_messages, repair_cfg)
        repaired_json_str = _extract_json(repaired)

        try:
            data = json.loads(repaired_json_str)
            with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ë¶„ì„ ì™„ë£Œ(ë³µêµ¬ ì„±ê³µ)! -> {OUTPUT_JSON}")
            return data
        except Exception:
            # ë””ë²„ê¹… ì €ì¥
            with open("raw_model_output.txt", "w", encoding="utf-8") as f:
                f.write(response)
            with open("raw_model_json_attempt.txt", "w", encoding="utf-8") as f:
                f.write(json_str)
            print("âŒ JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨.")
            print("ğŸ§¾ ì €ì¥ë¨: raw_model_output.txt / raw_model_json_attempt.txt")
            raise


# =========================
# íŠ¸ë¦¬ ì¶œë ¥(ìœ ì§€)
# =========================
def print_tree(node, prefix="", is_last=True):
    connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "

    if "method" in node:
        name = f"[{node['method']}] {node['url']} ({node.get('description', '')})"
    elif "function" in node:
        name = f"Æ’ {node['function']} - {node.get('description', '')}"
    elif "category" in node:
        name = f"ğŸ“‚ Category: {node.get('categoryName', node['category'])}"
    else:
        name = "Unknown Node"

    print(prefix + connector + name)

    children = node.get("children", [])
    if "endpoints" in node:
        children = node["endpoints"]

    count = len(children)
    for i, child in enumerate(children):
        new_prefix = prefix + ("    " if is_last else "â”‚   ")
        print_tree(child, new_prefix, i == count - 1)


def visualize_json_structure():
    if not os.path.exists(OUTPUT_JSON):
        print("âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(OUTPUT_JSON, "r", encoding="utf-8") as f:
        data = json.load(f)

    print("\nğŸŒ³ API Call Graph Analysis Result\n" + "=" * 40)
    if "api" in data:
        for cat in data["api"]:
            print_tree(cat)
    else:
        print("âš ï¸ 'api' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. JSON êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


# =========================
# CLI ì‹¤í–‰
# =========================
if __name__ == "__main__":
    cfg = AnalyzerConfig(
        max_total_lines=2000,  
        max_total_chars=50_000,
        max_time_seconds=200,
        max_new_tokens=4096,
        repetition_penalty=1.1
    )
    data = analyze_to_json(cfg)
    visualize_json_structure()
