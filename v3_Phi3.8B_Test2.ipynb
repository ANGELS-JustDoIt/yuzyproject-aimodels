{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a5ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì‹¤í–‰ í™˜ê²½: CUDA\n"
     ]
    }
   ],
   "source": [
    "# [Cell 1] ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ì„¤ì •ê°’\n",
    "INPUT_FILE = \"project_full_context.txt\"\n",
    "OUTPUT_JSON = \"project_flows.json\"\n",
    "\n",
    "# GPU í™•ì¸\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸš€ ì‹¤í–‰ í™˜ê²½: {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50f355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pak10\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-Coder-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# [Cell 2] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"  # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ID\n",
    "\n",
    "try:\n",
    "    if 'model' not in globals():\n",
    "        print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"eager\"\n",
    "        )\n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"âœ… ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38adbcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê³„ì¸µí˜• ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# [Cell 3] í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € (ê³„ì¸µí˜• íŠ¸ë¦¬ êµ¬ì¡° ì „ìš©)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a 'Backend Architecture Analyzer'.\n",
    "Your goal is to generate a **Hierarchical Call Graph** in JSON format for the provided codebase.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1. **Identify Categories**: Group endpoints by feature (e.g., Auth, Post, User).\n",
    "2. **Find Endpoints**: Look for Router definitions (e.g., `router.post('/signup', ...)`).\n",
    "3. **Trace Recursively**: For each endpoint, trace the function calls deeper and deeper.\n",
    "   - Router -> Middleware -> Controller -> Service/Data Layer -> Database/Library.\n",
    "4. **Structure**: Use the `children` array to represent nested function calls.\n",
    "\n",
    "### JSON OUTPUT FORMAT (STRICT)\n",
    "You MUST follow this exact structure. Do not output markdown code blocks, just the JSON.\n",
    "\n",
    "{\n",
    "  \"api\": [\n",
    "    {\n",
    "      \"category\": \"auth\",\n",
    "      \"categoryName\": \"Auth Feature\",\n",
    "      \"endpoints\": [\n",
    "        {\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"/auth/login\",\n",
    "          \"function\": \"login\",\n",
    "          \"file\": \"controller/auth.js\",\n",
    "          \"description\": \"User Login\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"function\": \"validateLogin\",\n",
    "              \"file\": \"middleware/valid.js\",\n",
    "              \"description\": \"Validation Middleware\",\n",
    "              \"children\": [\n",
    "                 { \"function\": \"isEmail()\", \"file\": \"express-validator\", \"description\": \"Check email format\", \"children\": [] }\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"function\": \"findByUserid\",\n",
    "              \"file\": \"data/auth.js\",\n",
    "              \"description\": \"Find user from DB\",\n",
    "              \"children\": [\n",
    "                 { \"function\": \"db.execute()\", \"file\": \"mysql2\", \"description\": \"SELECT * FROM users...\", \"children\": [] }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… ê³„ì¸µí˜• ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa8ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ 'project_full_context.txt' ì½ëŠ” ì¤‘...\n",
      "ğŸ§  AIê°€ ì „ì²´ êµ¬ì¡°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦½ë‹ˆë‹¤)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m json_data = \u001b[43mrun_full_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mrun_full_scan\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ì¶”ë¡ \u001b[39;00m\n\u001b[32m     35\u001b[39m input_ids = tokenizer.apply_chat_template(messages, tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# JSONì´ ê¸¸ê²Œ ë‚˜ì˜¤ë¯€ë¡œ í† í°ì„ ë„‰ë„‰í•˜ê²Œ ì¤Œ\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m][input_ids.shape[-\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# JSON íŒŒì‹± ë° ì €ì¥\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2016\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2017\u001b[39m         input_ids=input_ids,\n\u001b[32m   2018\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2019\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2020\u001b[39m         **model_kwargs,\n\u001b[32m   2021\u001b[39m     )\n\u001b[32m   2023\u001b[39m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2024\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2036\u001b[39m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[32m   2037\u001b[39m     prepared_logits_warper = (\n\u001b[32m   2038\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_logits_warper(generation_config, device=input_ids.device)\n\u001b[32m   2039\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m generation_config.do_sample\n\u001b[32m   2040\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2041\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3020\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[39m\n\u001b[32m   3018\u001b[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   3019\u001b[39m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3020\u001b[39m     next_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m   3021\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3022\u001b[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 4] ì „ì²´ ì½”ë“œ ë¶„ì„ ë° JSON ìƒì„± ì—”ì§„\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"AI ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ\"\"\"\n",
    "    text = re.sub(r\"^```(json)?\", \"\", text.strip(), flags=re.MULTILINE)\n",
    "    text = re.sub(r\"```$\", \"\", text.strip(), flags=re.MULTILINE)\n",
    "    \n",
    "    # ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start == -1 or end == -1: return \"{}\"\n",
    "    return text[start:end+1]\n",
    "\n",
    "def run_full_scan():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"âŒ '{INPUT_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“‚ '{INPUT_FILE}' ì½ëŠ” ì¤‘...\")\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        full_code = f.read()\n",
    "\n",
    "    # ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ 50,000ìë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€)\n",
    "    # Phi-3.5ë‚˜ Qwenì€ ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ ë” ëŠ˜ë ¤ë„ ë©ë‹ˆë‹¤.\n",
    "    code_context = full_code[:60000] \n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this entire codebase and generate the Nested JSON Structure:\\n\\n{code_context}\"}\n",
    "    ]\n",
    "\n",
    "    print(\"ğŸ§  AIê°€ ì „ì²´ êµ¬ì¡°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦½ë‹ˆë‹¤)\")\n",
    "    \n",
    "    # ì¶”ë¡ \n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=4000, # JSONì´ ê¸¸ê²Œ ë‚˜ì˜¤ë¯€ë¡œ í† í°ì„ ë„‰ë„‰í•˜ê²Œ ì¤Œ\n",
    "        temperature=0.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # JSON íŒŒì‹± ë° ì €ì¥\n",
    "    json_str = extract_json(response)\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… ì™„ë²½í•œ JSON íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! -> {OUTPUT_JSON}\")\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âŒ JSON íŒŒì‹± ì‹¤íŒ¨. AI ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        print(response[:500])\n",
    "        return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "json_data = run_full_scan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c43c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 'createJwtToken' ë°œê²¬! (ì½”ë“œ ìœ„ì¹˜: 667 ~ 4667 ì¶”ì¶œ)\n",
      "ğŸ§  AI ë¶„ì„ ì‹œì‘: 'createJwtToken' (ëª¨ë“œ: deep_dive)\n",
      "âœ… JSON ìƒì„± ì„±ê³µ! (project_flows.json)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Cell 5] í…ìŠ¤íŠ¸ íŠ¸ë¦¬ ë·°ì–´ (JSON êµ¬ì¡° í™•ì¸ìš©)\n",
    "\n",
    "def print_tree(node, prefix=\"\", is_last=True):\n",
    "    \"\"\"ì¬ê·€ì ìœ¼ë¡œ JSON íŠ¸ë¦¬ë¥¼ ì¶œë ¥\"\"\"\n",
    "    connector = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "    \n",
    "    # ë…¸ë“œ ì´ë¦„ ê²°ì • (function ë˜ëŠ” method+url)\n",
    "    if \"method\" in node:\n",
    "        name = f\"[{node['method']}] {node['url']} ({node.get('description', '')})\"\n",
    "    elif \"function\" in node:\n",
    "        name = f\"Æ’ {node['function']} - {node.get('description', '')}\"\n",
    "    elif \"category\" in node:\n",
    "        name = f\"ğŸ“‚ Category: {node.get('categoryName', node['category'])}\"\n",
    "    else:\n",
    "        name = \"Unknown Node\"\n",
    "\n",
    "    print(prefix + connector + name)\n",
    "    \n",
    "    # ìì‹ ë…¸ë“œ ìˆœíšŒ\n",
    "    children = node.get(\"children\", [])\n",
    "    if \"endpoints\" in node: # ì¹´í…Œê³ ë¦¬ ë ˆë²¨\n",
    "        children = node[\"endpoints\"]\n",
    "        \n",
    "    count = len(children)\n",
    "    for i, child in enumerate(children):\n",
    "        new_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "        print_tree(child, new_prefix, i == count - 1)\n",
    "\n",
    "def visualize_json_structure():\n",
    "    if not os.path.exists(OUTPUT_JSON):\n",
    "        print(\"âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\"\\nğŸŒ³ API Call Graph Analysis Result\\n\" + \"=\"*40)\n",
    "    if \"api\" in data:\n",
    "        for cat in data[\"api\"]:\n",
    "            print_tree(cat)\n",
    "    else:\n",
    "        print(\"âš ï¸ 'api' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. JSON êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "visualize_json_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11466b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ íŒŒì´ì¬ ì‹¤í–‰ ìœ„ì¹˜: c:\\Pyg\\Projects\\semi\\yuzyproject-aimodels\\venv\\Scripts\\python.exe\n",
      "------------------------------\n",
      "âŒ Transformers ë²„ì „: 4.57.3\n",
      "âŒ Accelerate ë²„ì „  : 1.12.0\n",
      "âŒ PyTorch ë²„ì „     : 2.9.1+cpu\n",
      "------------------------------\n",
      "â€» Transformers ë²„ì „ì´ '4.38.0' ë¯¸ë§Œì´ë©´ CMDì—ì„œ ì•„ë¬´ë¦¬ ì„¤ì¹˜í•´ë„ ì—¬ê¸°ì„  ì ìš© ì•ˆ ëœ ê²ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import transformers\n",
    "import accelerate\n",
    "import torch\n",
    "\n",
    "print(f\"ğŸ“‚ íŒŒì´ì¬ ì‹¤í–‰ ìœ„ì¹˜: {sys.executable}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"âŒ Transformers ë²„ì „: {transformers.__version__}\")\n",
    "print(f\"âŒ Accelerate ë²„ì „  : {accelerate.__version__}\")\n",
    "print(f\"âŒ PyTorch ë²„ì „     : {torch.__version__}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â€» Transformers ë²„ì „ì´ '4.38.0' ë¯¸ë§Œì´ë©´ CMDì—ì„œ ì•„ë¬´ë¦¬ ì„¤ì¹˜í•´ë„ ì—¬ê¸°ì„  ì ìš© ì•ˆ ëœ ê²ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c60f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672ca19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e2801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86bfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea9ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734fd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe0718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
